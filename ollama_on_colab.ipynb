{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install ollama\n",
    "`lshw` is necessary to install GPU support on Ubuntu.\n",
    "GPU support is not 'enabled'. If your run on GPU and `lshw` is installed when you install ollama, then ollama will natively run on GPU. Thus you can follow the rest of the notebook without changing anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y lshw\n",
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ollama serve in the background\n",
    "import subprocess\n",
    "\n",
    "pipe = subprocess.Pipe()\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    \"ollama serve\",\n",
    "    shell=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull mistral:7b-instruct-q4_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the model locally\n",
    "- generate will generate tokens, one JSON per token\n",
    "- chat will return a message\n",
    "- weirdly you sometimes need to restart using `ollama serve` again or you'll get the Err 111 connection refused error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:11434/api/generate -d '{ \\\n",
    "  \"model\": \"mistral:7b-instruct-q4_0\", \\\n",
    "  \"prompt\":\"Why is the sky blue?\" \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl http://localhost:11434/api/chat -d '{ \\\n",
    "  \"model\": \"mistral:7b-instruct-q4_0\", \\\n",
    "  \"messages\": [ \\\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" } \\\n",
    "  ] \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on langchain\n",
    "- install langchain\n",
    "- connect to ollama : just needs to rerun `ollama serve` sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "ollama = Ollama(base_url='http://localhost:11434',\n",
    "model=\"mistral:7b-instruct-q4_0\")\n",
    "print(ollama(\"why is the sky blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on llama-index\n",
    "- necessary to specifiy a timeout\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral:7b-instruct-q4_0\", base_url=\"http://localhost:11434\", request_timeout=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.complete(\"Who is Paul Graham?\")\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
