{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "Fastchat is a software backend designed to run LLMs.\n",
    "What we want to test in this notebook is :\n",
    "- run a single LLM on GPU in Google Colab\n",
    "- start an API server\n",
    "- query the API using the OpenAI package, indeed one of the features of fastchat is to use the OpenAI syntax for querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Download fastchat from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "%cd /content/\n",
    "\n",
    "# clone FastChat\n",
    "!git clone https://github.com/lm-sys/FastChat.git\n",
    "\n",
    "# install dependencies\n",
    "%cd FastChat\n",
    "!python3 -m pip install -e \".[model_worker,webui]\" --quiet\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "Fastchat uses 2 components to serve a model :\n",
    "1. Controller server which usually runs on 21001, acts as model backend\n",
    "2. Model worker which is managed by the controller\n",
    "\n",
    "That's for API mode and web app mode.\n",
    "\n",
    "Notes :\n",
    "- As colab is not printing out the standard output of the cells, you need to query the controller port before starting up the model worker. Or check using ps that the python controller is properly started.\n",
    "- The model worker may crash often : for instance if you cancel a running cell interacting with the model, that might kill the model worker. Unfortunately you almost always need to fully recreate the runtime environment to restart the model worker : force kill the session and restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that the controller is running\n",
    "!curl -X GET http://localhost:21001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check that the python processes are running\n",
    "!ps -e -o pid,pcpu,pmem,command | grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start controller\n",
    "controller_process = subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"127.0.0.1\"], stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you are sure that the controller is running, start the model worker\n",
    "# this process might take a while to start, therefore you should regularly run\n",
    "# `ps` to check that the worker is still running\n",
    "worker_process = subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.model_worker\", \"--host\", \"127.0.0.1\", \"--controller-address\", \"http://127.0.0.1:21001\", \"--model-path\", \"lmsys/vicuna-7b-v1.5\", \"--load-8bit\"], stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test that the port is responding (no connection error), use following command\n",
    "# If the port is listening, it should return `{\"detail\":\"Not Found\"}`\n",
    "!curl -X GET http://localhost:21001/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test that your worker is up and running, use\n",
    "!python3 -m fastchat.serve.test_message --model-name vicuna-7b-v1.5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, once you are sure that the worker is running, start the API server\n",
    "api_process = subprocess.Popen([\"python3\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"127.0.0.1\", \"--controller-address\", \"http://127.0.0.1:21001\", \"--port\", \"8000\"], stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the API server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a command to the API server\n",
    "!curl http://127.0.0.1:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \\\n",
    "    \"model\": \"vicuna-7b-v1.5\", \\\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, can you tell me a joke for me?\"}], \\\n",
    "    \"temperature\": 0.5 \\\n",
    "  }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use openai package to run queries\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.base_url = \"http://localhost:8000/v1/\"\n",
    "\n",
    "model = \"vicuna-7b-v1.5\"\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# create a completion\n",
    "completion = openai.completions.create(model=model, prompt=prompt, max_tokens=64)\n",
    "# print the completion\n",
    "print(prompt + completion.choices[0].text)\n",
    "\n",
    "# create a chat completion\n",
    "completion = openai.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n",
    ")\n",
    "# print the completion\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m fastchat.serve.gradio_web_server"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
